{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Routing between Capsules applied to S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basing our code on the model proposed in the paper [Dynamic Routing between capsules](https://arxiv.org/abs/1710.09829)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-04T04:12:49.723737Z",
     "start_time": "2018-06-04T04:12:46.778695Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Selim/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the tensorflow version 1.7.0\n",
    "\n",
    "A lot of this code was inspired by the amazing Github of Aur√©lien Geron :\n",
    "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-04T04:12:49.742826Z",
     "start_time": "2018-06-04T04:12:49.737786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the S&P 500 data that was preprocessed in a previous Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-04T04:12:49.767804Z",
     "start_time": "2018-06-04T04:12:49.757191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(982, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "# We try to get the shape of the ticker files from the S&P 500\n",
    "path = \"./data_out_autoreg/{}/\".format('AAPL')\n",
    "x_train = np.load(path+'X_train.npy')\n",
    "\n",
    "print(x_train.shape)\n",
    "shape_x_train = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we are using the parameters as proposed in the original paper.\n",
    "\n",
    "The __fix_params__ are the paramters that describe the input size of the image and the output size that we want to have. In this case we want an output that can have 2 different possibilities ( Up or Down ).\n",
    "\n",
    "__params__ are the parameters that define our convolutional layers and the size of the capsules in the capsule layer. We use two convolutions and the parameters for those convolutions have the suffix conv1 or conv2.\n",
    "\n",
    "__train_params__ include all the training parameters used in the training of our Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:18:19.268467Z",
     "start_time": "2018-04-26T00:18:19.259400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_params = {'tensor_width': shape_x_train[1], 'tensor_height': shape_x_train[2], 'tensor_depth': 1, 'n_output':2}\n",
    "\n",
    "params = {'n_filters': 32, \n",
    "          'strides_conv1': 1, 'strides_conv2': 1,\n",
    "         'kernel_size_conv1_1': 5,'kernel_size_conv1_2': 3, 'kernel_size_conv2_1': 1,'kernel_size_conv2_2': 1,\n",
    "         'caps1_n_maps': 16, 'caps1_n_dims': 2,\n",
    "         'caps2_n_dims': 8,\n",
    "         'init_sigma':0.1}\n",
    "\n",
    "train_params= {'m_plus': 0.9, 'm_minus': 0.1, 'lambda_': 0.5, 'mu': 0.5,\n",
    "                  'n_hidden1':512, 'n_hidden2': 1024,\n",
    "              'alpha': 0.005,\n",
    "              'n_epochs' : 20, 'batch_size' :2,\n",
    "              'drop_out_conv1':0.75, 'drop_out_conv2':1}\n",
    "\n",
    "ticker = 'AAPL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included a saver and Tensorboard to analyze the different parameters.\n",
    "We put the Model into a function with the parameters as input to use make it easier to change different parameters inside the model for developing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:27:49.785272Z",
     "start_time": "2018-04-26T00:27:44.296972Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def capsnetmodel_tb_save(train_parameters, fix_parameters, parameters, ticker):\n",
    "\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print('TICKER: {}'.format(ticker))\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    tf.reset_default_graph()\n",
    "    \"\"\"\n",
    "    This is the model that takes the different parametes as input. \n",
    "    Our data is always the same for our case hence we do not have it in our arguments.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Start variables for {}'.format(ticker))\n",
    "    # Fix parameters are loaded\n",
    "    n_output = fix_parameters['n_output']\n",
    "    tensor_width = fix_parameters['tensor_width']\n",
    "    tensor_height = fix_parameters['tensor_height']\n",
    "    tensor_depth = fix_parameters['tensor_depth']\n",
    "\n",
    "    # Optimizable parameters are loaded\n",
    "    n_filters = parameters['n_filters']\n",
    "    strides_conv1 = parameters['strides_conv1']\n",
    "    strides_conv2 = parameters['strides_conv2']\n",
    "    kernel_size_conv1_1 = parameters['kernel_size_conv1_1']\n",
    "    kernel_size_conv1_2 = parameters['kernel_size_conv1_2']\n",
    "    kernel_size_conv2_1 = parameters['kernel_size_conv2_1']\n",
    "    kernel_size_conv2_2 = parameters['kernel_size_conv2_2']\n",
    "\n",
    "    caps1_n_maps = parameters['caps1_n_maps']\n",
    "    caps1_n_dims = parameters['caps1_n_dims']\n",
    "    caps2_n_dims = parameters['caps2_n_dims']\n",
    "    conv_1_dropout = train_parameters['drop_out_conv1']\n",
    "    conv_2_dropout = train_parameters['drop_out_conv2']\n",
    "\n",
    "    init_sigma = parameters['init_sigma']\n",
    "\n",
    "    m_plus = train_parameters['m_plus']\n",
    "    m_minus = train_parameters['m_minus']\n",
    "    lambda_ = train_parameters['lambda_']\n",
    "    mu = train_parameters['mu']\n",
    "\n",
    "    print('Start input for {}'.format(ticker))\n",
    "    restore_checkpoint = True\n",
    "\n",
    "    path = './data_out_autoreg/{}/'.format(ticker)\n",
    "\n",
    "    # Data is loaded\n",
    "\n",
    "    x_train = np.load(path+'X_train.npy')\n",
    "    print(x_train.shape)\n",
    "    shape_x_train = x_train.shape\n",
    "    x_train = x_train.reshape(-1, shape_x_train[1]*shape_x_train[2])\n",
    "    print(x_train.shape)\n",
    "    x_test = np.load(path+'X_val.npy')\n",
    "    x_test = x_test.reshape(-1, shape_x_train[1]*shape_x_train[2])\n",
    "    y_train = np.load(path+'Y_train.npy')\n",
    "    y_test = np.load(path+'Y_val.npy')\n",
    "\n",
    "    # Decoder parameters are loaded\n",
    "\n",
    "    n_hidden1 = train_parameters['n_hidden1']\n",
    "    n_hidden2 = train_parameters['n_hidden2']\n",
    "    n_output_flat = fix_parameters['tensor_height'] * \\\n",
    "        fix_parameters['tensor_width'] * fix_parameters['tensor_depth']\n",
    "    alpha = train_parameters['alpha']\n",
    "\n",
    "    # Training parameters are loaded\n",
    "\n",
    "    n_epochs = train_parameters['n_epochs']\n",
    "    batch_size_train = train_parameters['batch_size']\n",
    "    n_iterations_per_epoch = len(x_train) // batch_size_train\n",
    "    n_iterations_validation = len(x_test) // batch_size_train\n",
    "    \n",
    "    # We initialize the best values to infinty for the loss and 0 for epoch/accuracies.\n",
    "    best_loss_val = np.infty\n",
    "    best_loss_train = np.infty\n",
    "\n",
    "    best_val_epoch = 0\n",
    "    best_train_epoch = 0\n",
    "\n",
    "    best_acc_train = 0\n",
    "    best_acc_val = 0\n",
    "\n",
    "    list_accuracy_val = list()\n",
    "    list_loss_val = list()\n",
    "\n",
    "    list_accuracy_train = list()\n",
    "    list_loss_train = list()\n",
    "\n",
    "    # Checkpoint for path\n",
    "    checkpoint_path = \"./board/capsule_checkpoint\"\n",
    "\n",
    "    print('Start model {}'.format(ticker))\n",
    "    # Our placeholders are the input X and the output Y.\n",
    "    X = tf.placeholder(tf.float32, shape=[\n",
    "                       None, tensor_width * tensor_height * tensor_depth], name='Input')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_output], name='Output')\n",
    "    x_image = tf.reshape(\n",
    "        X, [-1, tensor_width, tensor_height, tensor_depth], name='images')\n",
    "\n",
    "    # We use helper functions to define our Convolutional Layers\n",
    "    def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "        dtype = tf.float32\n",
    "        var = variable_on_cpu(\n",
    "            name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.multiply(\n",
    "                tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "        return var\n",
    "\n",
    "    #Function to use variable on cpu to free gpu space\n",
    "    def variable_on_cpu(name, shape, initializer):\n",
    "        with tf.device('/cpu:0'):\n",
    "            dtype = tf.float32\n",
    "            var = tf.get_variable(\n",
    "                name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "    \n",
    "    # The first convolutional Layer\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # We use the parameters above to define our kernel for the first convolutional layer\n",
    "        # We are using a convolution\n",
    "        kernel = variable_with_weight_decay('weights', shape=[\n",
    "                                            kernel_size_conv1_1, kernel_size_conv1_2, tensor_depth, n_filters], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(\n",
    "            x_image, kernel, [1, strides_conv1, strides_conv1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu(\n",
    "            'biases', [n_filters], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv1 = tf.nn.dropout(conv1, keep_prob=conv_1_dropout)\n",
    "        \n",
    "    # The second convolutional Layer\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        \n",
    "        # We use the parameters above to define our kernel for the second convolutional layer\n",
    "        kernel = variable_with_weight_decay(\n",
    "            'weights', shape=[kernel_size_conv2_1,kernel_size_conv2_2, n_filters, n_filters], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(\n",
    "            conv1, kernel, [1, strides_conv2, strides_conv2, 1], padding='SAME')\n",
    "        biases = variable_on_cpu(\n",
    "            'biases', [n_filters], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv2 = tf.nn.dropout(conv2, keep_prob=conv_2_dropout)\n",
    "\n",
    "    # We take the dimensions of the second convolutional layer to connect it correctly to our capsules layer\n",
    "    dim = conv2.get_shape().as_list()\n",
    "\n",
    "    # Number of capsules in our layer is the number of maps we want times the two dimension of our layer\n",
    "    caps1_n_caps = caps1_n_maps * dim[1] * dim[2]\n",
    "\n",
    "    # Reshape of the convolutional layers into a capusle layer with the number of capsules calculated in the previous line.\n",
    "    caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                           name=\"caps1_raw\")\n",
    "    \n",
    "    # Implementation of the Squash function used in the paper to normalize the output of the first capsule layer.\n",
    "    def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "        with tf.name_scope(name, default_name=\"squash\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keepdims=True)\n",
    "            safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = s / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "\n",
    "    caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "\n",
    "    caps2_n_caps = n_output\n",
    "\n",
    "    # W is the Weight Matrix from the paper\n",
    "    W_init = tf.random_normal(\n",
    "        shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "        stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "    W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    \n",
    "    # To correctly multiply the weight matrix with the output of Capsule Layer 1 \n",
    "    # we expand(duplicate) the tensor along its axis.\n",
    "    W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "    caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                           name=\"caps1_output_expanded\")\n",
    "    caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                       name=\"caps1_output_tile\")\n",
    "    caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                                 name=\"caps1_output_tiled\")\n",
    "\n",
    "    # Now we can correclty multiply the Weight matrix and the output of the first capsule layer\n",
    "    caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                                name=\"caps2_predicted\")\n",
    "\n",
    "    #Start of the Routing by Agreement Algorithm\n",
    "    \n",
    "    #Initialize the weights\n",
    "    raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                           dtype=np.float32, name=\"raw_weights\")\n",
    "\n",
    "    # We use two rounds of routing. In our case those two rounds are hard coded.\n",
    "    # Round 1\n",
    "    routing_weights = tf.nn.softmax(\n",
    "        raw_weights, axis=2, name=\"routing_weights\")\n",
    "\n",
    "    weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                       name=\"weighted_predictions\")\n",
    "    weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True,\n",
    "                                 name=\"weighted_sum\")\n",
    "\n",
    "    caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                                  name=\"caps2_output_round_1\")\n",
    "\n",
    "    # Round 2\n",
    "    #Expand the output by the number of capsules in the first layers\n",
    "    caps2_output_round_1_tiled = tf.tile(\n",
    "        caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "        name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "    # Update of the weights for round 2 as explained in the paper\n",
    "    agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                          transpose_a=True, name=\"agreement\")\n",
    "\n",
    "    \n",
    "    raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
    "                                 name=\"raw_weights_round_2\")\n",
    "\n",
    "    # Similar to round 1\n",
    "    routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                            axis=2,\n",
    "                                            name=\"routing_weights_round_2\")\n",
    "    weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                               caps2_predicted,\n",
    "                                               name=\"weighted_predictions_round_2\")\n",
    "    weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                         axis=1, keepdims=True,\n",
    "                                         name=\"weighted_sum_round_2\")\n",
    "    caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                                  axis=-2,\n",
    "                                  name=\"caps2_output_round_2\")\n",
    "\n",
    "    caps2_output = caps2_output_round_2\n",
    "\n",
    "    # Special Norm Function to avoid division by 0\n",
    "    def safe_norm(s, axis=-1, epsilon=1e-7, keepdims=False, name=None):\n",
    "        with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keepdims=keepdims)\n",
    "            return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "    # The output probabilities are given by normalizing the output of the last capsule layer after round 2\n",
    "    y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "    y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "    y_pred = tf.squeeze(y_proba_argmax, axis=[1, 2], name=\"y_pred\")\n",
    "\n",
    "    # After predicting y we calculate the Loss with the Loss function specified in the paper\n",
    "    print('Start Loss for {}'.format(ticker))\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "    T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "    caps2_output_norm = safe_norm(caps2_output, axis=-2, keepdims=True,\n",
    "                                  name=\"caps2_output_norm\")\n",
    "\n",
    "    present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                                  name=\"present_error_raw\")\n",
    "    present_error = tf.reshape(present_error_raw, shape=(-1, n_output),\n",
    "                               name=\"present_error\")\n",
    "\n",
    "    absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                                 name=\"absent_error_raw\")\n",
    "    absent_error = tf.reshape(absent_error_raw, shape=(-1, n_output),\n",
    "                              name=\"absent_error\")\n",
    "\n",
    "    # This is the actual Loss function\n",
    "    L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "               name=\"L\")\n",
    "\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "    tf.summary.scalar('margin_loss', margin_loss)\n",
    "\n",
    "    # Reconstruction of the input with the decoder\n",
    "    #Our mask is the vector where only the correct label is True\n",
    "    mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                                   name=\"mask_with_labels\")\n",
    "\n",
    "    \n",
    "    reconstruction_targets = tf.cond(mask_with_labels,  # condition\n",
    "                                     lambda: y,        # if True\n",
    "                                     lambda: y_pred,   # if False\n",
    "                                     name=\"reconstruction_targets\")\n",
    "\n",
    "    reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                     depth=caps2_n_caps,\n",
    "                                     name=\"reconstruction_mask\")\n",
    "\n",
    "    reconstruction_mask_reshaped = tf.reshape(\n",
    "        reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "        name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "    caps2_output_masked = tf.multiply(\n",
    "        caps2_output, reconstruction_mask_reshaped,\n",
    "        name=\"caps2_output_masked\")\n",
    "\n",
    "    decoder_input = tf.reshape(caps2_output_masked,\n",
    "                               [-1, caps2_n_caps * caps2_n_dims],\n",
    "                               name=\"decoder_input\")\n",
    "\n",
    "    # Decoder with three fully connected layers\n",
    "    print('Start decoder for {}'.format(ticker))\n",
    "    with tf.name_scope(\"decoder\"):\n",
    "        hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name=\"hidden1\")\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name=\"hidden2\")\n",
    "        decoder_output = tf.layers.dense(hidden2, n_output_flat,\n",
    "                                         activation=tf.nn.sigmoid,\n",
    "                                         name=\"decoder_output\")\n",
    "\n",
    "    # Reconstruction Loss function\n",
    "    print('start reconstruction loss')\n",
    "\n",
    "    # Flatten X\n",
    "    X_flat = tf.reshape(X, [-1, n_output_flat], name=\"X_flat\")\n",
    "    \n",
    "    # We take the squared difference between the original input and the reconstructed one\n",
    "    squared_difference = tf.square(X_flat - decoder_output,\n",
    "                                   name=\"squared_difference\")\n",
    "    reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                         name=\"reconstruction_loss\")\n",
    "\n",
    "    # The loss function of our model is composed of the  reconstruction loss function (regularizer) \n",
    "    # and first loss function.\n",
    "    \n",
    "    loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # We use the AdamOptimizer for this model and we want to minimize the Loss function specified earlier\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #summaries_dir = \"./board/summaries\"\n",
    "    #graph_dir = './board/graphs'\n",
    "    #merged = tf.summary.merge_all()\n",
    "\n",
    "    print('Start training')\n",
    "\n",
    "    # All the summaries and graphs are saved to those paths.\n",
    "    summaries_dir = \"./board/summaries\"\n",
    "\n",
    "    graph_dir = './board/graphs'\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "    # Start of our tensorflow session \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(summaries_dir + '/train')\n",
    "        test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "        graph_writer = tf.summary.FileWriter(graph_dir, sess.graph)\n",
    "\n",
    "#         if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "#             saver.restore(sess, checkpoint_path)\n",
    "#             print(\"Starting from checkpoint\")\n",
    "#         else:\n",
    "        init.run()\n",
    "        print(\"Starting from scratch: {}\".format(ticker))\n",
    "\n",
    "        # Training of the model starts here.\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('TRAINING EPOCH: {}'.format(epoch))\n",
    "            acc_trains = []\n",
    "            for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "                X_batch = x_train[(iteration-1) *\n",
    "                                  batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_train[(iteration-1) *\n",
    "                                  batch_size_train:iteration*batch_size_train]\n",
    "                \n",
    "                # Run the training operation and measure the loss:\n",
    "\n",
    "                summary, _, loss_train, acc_tra = sess.run(\n",
    "                    [merged, training_op, loss, accuracy],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch,\n",
    "                               mask_with_labels: True})\n",
    "                train_writer.add_summary(\n",
    "                    summary, iteration + n_iterations_per_epoch*epoch)\n",
    "                acc_trains.append(acc_tra)\n",
    "                print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                    iteration, n_iterations_per_epoch,\n",
    "                    iteration * 100 / n_iterations_per_epoch,\n",
    "                    loss_train),\n",
    "                    end=\"\")\n",
    "            acc_trains = np.average(acc_trains)\n",
    "            print('\\nTrain Accuracy {:.4f}%'.format(acc_trains*100))\n",
    "\n",
    "            \n",
    "            # Testing starts here\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('TESTING EPOCH: {}'.format(epoch))\n",
    "            \n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals = []\n",
    "            acc_vals = []\n",
    "            outputs = np.zeros([])\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                # .reshape([-1,tensor_depth,tensor_width, tensor_height]).transpose([0,3,2,1]).reshape(-1, tensor_depth*tensor_width*tensor_height)\n",
    "                X_batch = x_test[(iteration-1) *\n",
    "                                 batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test[(iteration-1) *\n",
    "                                 batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test, loss_val, acc_val, output = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer.add_summary(\n",
    "                    summary_test, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals.append(loss_val)\n",
    "                acc_vals.append(acc_val)\n",
    "                outputs = np.append(outputs, output)\n",
    "                print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val = np.mean(loss_vals)\n",
    "            acc_val = np.mean(acc_vals)\n",
    "            print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val * 100, loss_val,\n",
    "                \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "            # We save the model given different best accuracies and loss values and their epochs\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, checkpoint_path)\n",
    "                best_loss_val = loss_val\n",
    "\n",
    "            if acc_val > best_acc_val:\n",
    "\n",
    "                best_acc_val = acc_val\n",
    "                best_val_epoch = epoch\n",
    "\n",
    "            if acc_trains > best_acc_train:\n",
    "                #save_path = saver.save(sess, checkpoint_path)\n",
    "                best_loss_train = loss_train\n",
    "                best_train_epoch = epoch\n",
    "                best_acc_train = acc_trains\n",
    "\n",
    "            list_loss_val.append(loss_val)\n",
    "            list_accuracy_val.append(acc_val)\n",
    "            list_loss_train.append(loss_train)\n",
    "            list_accuracy_train.append(acc_trains)\n",
    "\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        graph_writer.close()\n",
    "\n",
    "        print('ticker:', ticker, 'max_train_accuracy:',\n",
    "              round(100*best_acc_train, 2), '%')\n",
    "        print('ticker:', ticker, 'max_val_accuracy:',\n",
    "              round(100*best_acc_val, 2), '%')\n",
    "\n",
    "        print('ticker:', ticker, 'Mean Train Accuracy over 20 epochs:',\n",
    "              round(100*np.mean(list_accuracy_train), 2), '%')\n",
    "        print('ticker:', ticker, 'Mean Train Loss over 20 epochs:',\n",
    "              np.mean(list_loss_train))\n",
    "        print('ticker:', ticker, 'Mean Validation Accuracy over 20 epochs:',\n",
    "              round(100*np.mean(list_accuracy_val), 2), '%')\n",
    "        print('ticker:', ticker, 'Mean Validation Loss over 20 epochs:',\n",
    "              np.mean(list_loss_val))\n",
    "\n",
    "        return {'ticker': ticker, 'max_train_accuracy': best_acc_train, 'epoch_max_train': best_train_epoch,\n",
    "                'max_val_accuracy': best_acc_val, 'epoch_max_val': best_val_epoch, 'keep_prob1': conv_1_dropout,\n",
    "                'keep_prob2': conv_2_dropout}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:33:31.850811Z",
     "start_time": "2018-04-26T00:27:50.051534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "TICKER: AAPL\n",
      "------------------------------------------------------------------------------\n",
      "start variablesAAPL\n",
      "start input AAPL\n",
      "(982, 30, 5)\n",
      "(982, 150)\n",
      "start model AAPL\n",
      "start loss AAPL\n",
      "start decoder AAPL\n",
      "start reconstruction loss\n",
      "start training\n",
      "Starting from scratch: AAPL\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.21897\n",
      "Train Accuracy 51.5275%\n",
      "Epoch: 1  Val accuracy: 46.7213%  Loss: 0.219104 (improved)\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.21396\n",
      "Train Accuracy 51.7312%\n",
      "Epoch: 2  Val accuracy: 50.8197%  Loss: 0.219448\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.20418\n",
      "Train Accuracy 53.0550%\n",
      "Epoch: 3  Val accuracy: 45.9016%  Loss: 0.219980\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.18679\n",
      "Train Accuracy 53.7678%\n",
      "Epoch: 4  Val accuracy: 45.0820%  Loss: 0.225207\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.18229\n",
      "Train Accuracy 58.3503%\n",
      "Epoch: 5  Val accuracy: 40.9836%  Loss: 0.229354\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.20099\n",
      "Train Accuracy 60.7943%\n",
      "Epoch: 6  Val accuracy: 45.0820%  Loss: 0.236812\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.19675\n",
      "Train Accuracy 62.7291%\n",
      "Epoch: 7  Val accuracy: 44.2623%  Loss: 0.253914\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.21366\n",
      "Train Accuracy 66.0896%\n",
      "Epoch: 8  Val accuracy: 42.6230%  Loss: 0.254862\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.24857\n",
      "Train Accuracy 67.9226%\n",
      "Epoch: 9  Val accuracy: 46.7213%  Loss: 0.257474\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.16891\n",
      "Train Accuracy 70.2648%\n",
      "Epoch: 10  Val accuracy: 48.3607%  Loss: 0.256747\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.24040\n",
      "Train Accuracy 75.5601%\n",
      "Epoch: 11  Val accuracy: 44.2623%  Loss: 0.262476\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.20872\n",
      "Train Accuracy 80.1426%\n",
      "Epoch: 12  Val accuracy: 46.7213%  Loss: 0.253636\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.15843\n",
      "Train Accuracy 84.2159%\n",
      "Epoch: 13  Val accuracy: 49.1803%  Loss: 0.260282\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.14024\n",
      "Train Accuracy 85.6415%\n",
      "Epoch: 14  Val accuracy: 49.1803%  Loss: 0.252871\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.13027\n",
      "Train Accuracy 89.1039%\n",
      "Epoch: 15  Val accuracy: 48.3607%  Loss: 0.250797\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.11909\n",
      "Train Accuracy 90.7332%\n",
      "Epoch: 16  Val accuracy: 44.2623%  Loss: 0.262311\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.10414\n",
      "Train Accuracy 93.1772%\n",
      "Epoch: 17  Val accuracy: 53.2787%  Loss: 0.247540\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.12719\n",
      "Train Accuracy 93.3809%\n",
      "Epoch: 18  Val accuracy: 52.4590%  Loss: 0.241023\n",
      "Iteration: 491/491 (100.0%)  Loss: 0.09224\n",
      "Train Accuracy 94.9084%\n",
      "Epoch: 19  Val accuracy: 55.7377%  Loss: 0.236286      \n",
      "Iteration: 491/491 (100.0%)  Loss: 0.09356\n",
      "Train Accuracy 95.1120%\n",
      "Epoch: 20  Val accuracy: 50.8197%  Loss: 0.251260\n",
      "ticker: AAPL max_train_accuracy: 95.11 %\n",
      "ticker: AAPL max_val_accuracy: 55.74 %\n",
      "ticker: AAPL Mean Train Accuracy over 20 epochs: 73.91 %\n",
      "ticker: AAPL Mean Train Loss over 20 epochs: 0.1724684\n",
      "ticker: AAPL Mean Validation Accuracy over 20 epochs: 47.54 %\n",
      "ticker: AAPL Mean Validation Loss over 20 epochs: 0.24456906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch_max_train': 19,\n",
       " 'epoch_max_val': 18,\n",
       " 'keep_prob1': 0.75,\n",
       " 'keep_prob2': 1,\n",
       " 'max_train_accuracy': 0.95112014,\n",
       " 'max_val_accuracy': 0.55737704,\n",
       " 'ticker': 'AAPL'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsnetmodel_tb_save(train_params, fix_params, params, 'AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for all the tickers in our list and append the results as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T00:18:02.203117Z",
     "start_time": "2018-04-26T00:16:50.238Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './data_out_autoreg/'\n",
    "directory = data_dir # path to the files\n",
    "list_tickers  = os.listdir(directory) #these are the differents pdf files\n",
    "list_results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T20:49:53.044610Z",
     "start_time": "2018-04-25T20:49:02.638670Z"
    }
   },
   "outputs": [],
   "source": [
    "for ticker in list_tickers[12:15]:\n",
    "    list_results.append(capsnetmodel_tb_save(train_params, fix_params, params, ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
