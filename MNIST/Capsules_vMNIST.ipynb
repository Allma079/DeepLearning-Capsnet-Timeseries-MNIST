{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Routing between Capsules for the MNIST Data-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basing our code on the model proposed in the paper [Dynamic Routing between capsules](https://arxiv.org/abs/1710.09829)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:07:03.731749Z",
     "start_time": "2018-06-03T21:07:03.724909Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the tensorflow version 1.7.0\n",
    "\n",
    "A lot of this code was inspired by the amazing Github of Aur√©lien Geron :\n",
    "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:07:04.167540Z",
     "start_time": "2018-06-03T21:07:04.163620Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/exelban/tensorflow-cifar-10/blob/master/include/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the inbuilt MNIST data-set from Tensorflow and download it into the data_out folder. We do not train our model on other data than the normal MNIST train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:07:05.435443Z",
     "start_time": "2018-06-03T21:07:04.808157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000,)\n"
     ]
    }
   ],
   "source": [
    "restore_checkpoint = True\n",
    "path = \"./data_out/\"\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# Train Set\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotated MNIST Data-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following data sets from the LISA MNIST webpage: http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/MnistVariations\n",
    "\n",
    "- MNIST basic\n",
    "- MNIST + background images\n",
    "- MNIST + random background\n",
    "- Rotated MNIST digits\n",
    "- Rotated MNIST digits + background images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the downloaded data into a folder called data_transfo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:07:07.570902Z",
     "start_time": "2018-06-03T21:07:07.567913Z"
    }
   },
   "outputs": [],
   "source": [
    "path_test = './data_transfo/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:02:36.631884Z",
     "start_time": "2018-06-03T21:01:37.695Z"
    }
   },
   "source": [
    "Load the data into variables with numpy.loadtext ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:09:25.989851Z",
     "start_time": "2018-06-03T21:07:08.420138Z"
    }
   },
   "outputs": [],
   "source": [
    "test_normal = np.loadtxt(path_test + 'mnist_test.amat')\n",
    "test_rotated = np.loadtxt(path_test + 'mnist_all_rotation_normalized_float_test.amat')\n",
    "test_back_rand = np.loadtxt(path_test + 'mnist_background_random_test.amat')\n",
    "test_back_img = np.loadtxt(path_test + 'mnist_background_images_test.amat')\n",
    "test_rot_back_img = np.loadtxt(path_test + 'mnist_all_background_images_rotation_normalized_test.amat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to trim the data set to have the correct size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T21:09:26.011144Z",
     "start_time": "2018-06-03T21:09:25.992631Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test = test_normal[:,:-1]\n",
    "y_test = test_normal[:,-1]\n",
    "\n",
    "x_test_rotated = test_rotated[:,:-1]\n",
    "y_test_rotated = test_rotated[:,-1]\n",
    "\n",
    "x_test_back_rand = test_back_rand[:,:-1]\n",
    "y_test_back_rand = test_back_rand[:,-1]\n",
    "\n",
    "x_test_back_img = test_back_img[:,:-1]\n",
    "y_test_back_img = test_back_img[:,-1]\n",
    "\n",
    "x_test_rot_back_img = test_rot_back_img[:,:-1]\n",
    "y_test_rot_back_img = test_rot_back_img[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:01:44.963692Z",
     "start_time": "2018-04-25T15:01:44.955607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n",
      "(12000, 784) (12000,)\n",
      "(12000, 784) (12000,)\n",
      "(12000, 784) (12000,)\n",
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape, y_test.shape)\n",
    "print(x_test_rotated.shape, y_test_rotated.shape)\n",
    "print(x_test_back_rand.shape, y_test_back_rand.shape)\n",
    "print(x_test_back_img.shape, y_test_back_img.shape)\n",
    "print(x_test_rot_back_img.shape, y_test_rot_back_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters used for the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we are using the parameters as proposed in the original paper.\n",
    "\n",
    "The __fix_params__ are the paramters that describe the input size of the image and the output size that we want to have. In this case we want an output that can have 10 different possibilities ( from 0 to 9 ).\n",
    "\n",
    "__params__ are the parameters that define our convolutional layers and the size of the capsules in the capsule layer. We use two convolutions and the parameters for those convolutions have the suffix conv1 or conv2.\n",
    "\n",
    "__train_params__ include all the training parameters used in the training of our Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T22:31:10.841742Z",
     "start_time": "2018-06-03T22:31:10.834207Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_params = {'tensor_width': 28, 'tensor_height': 28, 'tensor_depth': 1, 'n_output':10}\n",
    "\n",
    "params = {'n_filters': 256, \n",
    "          'strides_conv1': 1, 'strides_conv2': 2,\n",
    "         'kernel_size_conv1': 9, 'kernel_size_conv2': 9,\n",
    "         'caps1_n_maps': 32, 'caps1_n_dims': 8,\n",
    "         'caps2_n_dims': 16,\n",
    "         'init_sigma':0.1}\n",
    "\n",
    "train_params= {'m_plus': 0.9, 'm_minus': 0.1, 'lambda_': 0.5, 'mu': 0.5,\n",
    "                  'n_hidden1':512, 'n_hidden2': 1024,\n",
    "              'alpha': 0.005,\n",
    "              'n_epochs' : 100, 'batch_size' :2,\n",
    "              'drop_out_conv1':1, 'drop_out_conv2':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included a saver and Tensorboard to analyze the different parameters.\n",
    "We put the Model into a function with the parameters as input to use make it easier to change different parameters inside the model for developing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T22:31:19.880694Z",
     "start_time": "2018-06-03T22:31:11.656247Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def capsnetmodel_tb_save(train_parameters, fix_parameters, parameters):\n",
    "    \"\"\"\n",
    "    This is the model that takes the different parametes as input. \n",
    "    Our data is always the same for our case hence we do not have it in our arguments.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Start Variables')\n",
    "\n",
    "    # Fix parameters are loaded\n",
    "    n_output = fix_parameters['n_output']\n",
    "    tensor_width = fix_parameters['tensor_width']\n",
    "    tensor_height = fix_parameters['tensor_height']\n",
    "    tensor_depth = fix_parameters['tensor_depth']\n",
    "\n",
    "    # Optimizable parameters are loaded\n",
    "    n_filters = parameters['n_filters']\n",
    "    strides_conv1 = parameters['strides_conv1']\n",
    "    strides_conv2 = parameters['strides_conv2']\n",
    "    kernel_size_conv1 = parameters['kernel_size_conv1']\n",
    "    kernel_size_conv2 = parameters['kernel_size_conv2']\n",
    "\n",
    "    caps1_n_maps = parameters['caps1_n_maps']\n",
    "    caps1_n_dims = parameters['caps1_n_dims']\n",
    "    caps2_n_dims = parameters['caps2_n_dims']\n",
    "    conv_1_dropout = train_parameters['drop_out_conv1']\n",
    "    conv_2_dropout = train_parameters['drop_out_conv2']\n",
    "\n",
    "    init_sigma = parameters['init_sigma']\n",
    "\n",
    "    m_plus = train_parameters['m_plus']\n",
    "    m_minus = train_parameters['m_minus']\n",
    "    lambda_ = train_parameters['lambda_']\n",
    "    mu = train_parameters['mu']\n",
    "\n",
    "    # Decoder parameters are loaded\n",
    "\n",
    "    n_hidden1 = train_parameters['n_hidden1']\n",
    "    n_hidden2 = train_parameters['n_hidden2']\n",
    "    n_output_flat = fix_parameters['tensor_height'] * \\\n",
    "        fix_parameters['tensor_width'] * fix_parameters['tensor_depth']\n",
    "    alpha = train_parameters['alpha']\n",
    "\n",
    "    # Training parameters are loaded\n",
    "\n",
    "    n_epochs = train_parameters['n_epochs']\n",
    "    batch_size_train = train_parameters['batch_size']\n",
    "    n_iterations_per_epoch = len(x_train) // batch_size_train\n",
    "    n_iterations_validation = len(x_test) // batch_size_train\n",
    "    best_loss_val = np.infty\n",
    "    best_loss_val_rotated = np.infty\n",
    "    best_loss_val_back_rand = np.infty\n",
    "    best_loss_val_back_img = np.infty\n",
    "    best_loss_val_rot_back_img = np.infty\n",
    "\n",
    "    # Path for the Save-Checkpoint\n",
    "    checkpoint_path = \"./board/capsule_checkpoint\"\n",
    "\n",
    "    print('Start Model')\n",
    "\n",
    "    # Our placeholders are the input X and the output Y.\n",
    "    X = tf.placeholder(tf.float32, shape=[\n",
    "                       None, tensor_width * tensor_height * tensor_depth], name='Input')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_output], name='Output')\n",
    "\n",
    "    # We need to reshape the image to have the correct dimensions.\n",
    "    x_image = tf.reshape(\n",
    "        X, [-1, tensor_width, tensor_height, tensor_depth], name='images')\n",
    "\n",
    "    # We use helper functions to define our Convolutional Layers\n",
    "    def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "        dtype = tf.float32\n",
    "        var = variable_on_cpu(\n",
    "            name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.multiply(\n",
    "                tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "        return var\n",
    "\n",
    "    def variable_on_cpu(name, shape, initializer):\n",
    "        with tf.device('/cpu:0'):\n",
    "            dtype = tf.float32\n",
    "            var = tf.get_variable(\n",
    "                name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    # The first convolutional Layer\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        \n",
    "        # We use the parameters above to define our kernel for the first convolutional layer\n",
    "        kernel = variable_with_weight_decay('weights', shape=[\n",
    "                                            kernel_size_conv1, kernel_size_conv1, tensor_depth, n_filters], stddev=5e-2, wd=0.0)\n",
    "        \n",
    "        conv = tf.nn.conv2d(\n",
    "            x_image, kernel, [1, strides_conv1, strides_conv1, 1], padding='SAME')\n",
    "        biases = variable_on_cpu(\n",
    "            'biases', [n_filters], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv1 = tf.nn.dropout(conv1, keep_prob=conv_1_dropout)\n",
    "\n",
    "    # The second convolutional Layer\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        \n",
    "        # We use the parameters above to define our kernel for the second convolutional layer\n",
    "        kernel = variable_with_weight_decay(\n",
    "            'weights', shape=[kernel_size_conv2, kernel_size_conv2, n_filters, n_filters], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(\n",
    "            conv1, kernel, [1, strides_conv2, strides_conv2, 1], padding='SAME')\n",
    "        biases = variable_on_cpu(\n",
    "            'biases', [n_filters], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        conv2 = tf.nn.dropout(conv2, keep_prob=conv_2_dropout)\n",
    "        \n",
    "    \n",
    "    # We take the dimensions of the second convolutional layer to connect it correctly to our capsules layer\n",
    "    dim = conv2.get_shape().as_list()\n",
    "\n",
    "    # Number of capsules in our layer is the number of maps we want times the two dimension of our layer\n",
    "    caps1_n_caps = caps1_n_maps * dim[1] * dim[2]\n",
    "    \n",
    "    # Reshape of the convolutional layers into a capusle layer with the number of capsules calculated in the previous line.\n",
    "    caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                           name=\"caps1_raw\")\n",
    "\n",
    "    # Implementation of the Squash function used in the paper to normalize the output of the first capsule layer.\n",
    "    def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "        with tf.name_scope(name, default_name=\"squash\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keepdims=True)\n",
    "            safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = s / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "\n",
    "    caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "    \n",
    "    caps2_n_caps = n_output\n",
    "    \n",
    "    # W is the Weight Matrix from the paper\n",
    "    W_init = tf.random_normal(\n",
    "        shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "        stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "    W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "    \n",
    "    # To correctly multiply the weight matrix with the output of Capsule Layer 1 \n",
    "    # we expand(duplicate) the tensor along its axis.\n",
    "    caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                           name=\"caps1_output_expanded\")\n",
    "    caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                       name=\"caps1_output_tile\")\n",
    "    caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                                 name=\"caps1_output_tiled\")\n",
    "\n",
    "    # Now we can correclty multiply the Weight matrix and the output of the first capsule layer\n",
    "    caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                                name=\"caps2_predicted\")\n",
    "    \n",
    "    #Start of the Routing by Agreement Algorithm\n",
    "    \n",
    "    #Initialize the weights\n",
    "    raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                           dtype=np.float32, name=\"raw_weights\")\n",
    "    \n",
    "\n",
    "    # We use two rounds of routing. In our case those two rounds are hard coded.\n",
    "    # Round 1\n",
    "    routing_weights = tf.nn.softmax(\n",
    "        raw_weights, axis=2, name=\"routing_weights\")\n",
    "\n",
    "    weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                       name=\"weighted_predictions\")\n",
    "    weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True,\n",
    "                                 name=\"weighted_sum\")\n",
    "\n",
    "    caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                                  name=\"caps2_output_round_1\")\n",
    "    \n",
    "    # Round 2\n",
    "    #Expand the output by the number of capsules in the first layers\n",
    "    caps2_output_round_1_tiled = tf.tile(\n",
    "        caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "        name=\"caps2_output_round_1_tiled\")\n",
    "    \n",
    "    # Update of the weights for round 2 as explained in the paper\n",
    "    agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                          transpose_a=True, name=\"agreement\")\n",
    "\n",
    "    raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
    "                                 name=\"raw_weights_round_2\")\n",
    "    \n",
    "    # Similar to round 1\n",
    "    routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                            axis=2,\n",
    "                                            name=\"routing_weights_round_2\")\n",
    "    weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                               caps2_predicted,\n",
    "                                               name=\"weighted_predictions_round_2\")\n",
    "    weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                         axis=1, keepdims=True,\n",
    "                                         name=\"weighted_sum_round_2\")\n",
    "    caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                                  axis=-2,\n",
    "                                  name=\"caps2_output_round_2\")\n",
    "\n",
    "    caps2_output = caps2_output_round_2\n",
    "    \n",
    "    # Special Norm Function to avoid division by 0\n",
    "    def safe_norm(s, axis=-1, epsilon=1e-7, keepdims=False, name=None):\n",
    "        with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keepdims=keepdims)\n",
    "            return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "    # The output probabilities are given by normalizing the output of the last capsule layer after round 2\n",
    "    y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "    y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "\n",
    "    y_pred = tf.squeeze(y_proba_argmax, axis=[1, 2], name=\"y_pred\")\n",
    "\n",
    "    \n",
    "    # After predicting y we calculate the Loss with the Loss function specified in the paper\n",
    "    print('Start Loss')\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "    T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "    caps2_output_norm = safe_norm(caps2_output, axis=-2, keepdims=True,\n",
    "                                  name=\"caps2_output_norm\")\n",
    "\n",
    "    present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                                  name=\"present_error_raw\")\n",
    "    present_error = tf.reshape(present_error_raw, shape=(-1, n_output),\n",
    "                               name=\"present_error\")\n",
    "\n",
    "    absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                                 name=\"absent_error_raw\")\n",
    "    absent_error = tf.reshape(absent_error_raw, shape=(-1, n_output),\n",
    "                              name=\"absent_error\")\n",
    "    \n",
    "    # This is the actual Loss function\n",
    "    L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "               name=\"L\")\n",
    "\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "    tf.summary.scalar('margin_loss', margin_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Reconstruction of the input with the decoder\n",
    "    #Our mask is the vector where only the correct label is True\n",
    "    mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                                   name=\"mask_with_labels\")\n",
    "\n",
    "    reconstruction_targets = tf.cond(mask_with_labels,  # condition\n",
    "                                     lambda: y,        # if True\n",
    "                                     lambda: y_pred,   # if False\n",
    "                                     name=\"reconstruction_targets\")\n",
    "\n",
    "    reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                     depth = caps2_n_caps,\n",
    "                                     name=\"reconstruction_mask\")\n",
    "\n",
    "    reconstruction_mask_reshaped = tf.reshape(\n",
    "        reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "        name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "    caps2_output_masked = tf.multiply(\n",
    "        caps2_output, reconstruction_mask_reshaped,\n",
    "        name=\"caps2_output_masked\")\n",
    "\n",
    "    decoder_input = tf.reshape(caps2_output_masked,\n",
    "                               [-1, caps2_n_caps * caps2_n_dims],\n",
    "                               name=\"decoder_input\")\n",
    "\n",
    "    # Decoder with three fully connected layers\n",
    "    print('start decoder')\n",
    "    with tf.name_scope(\"decoder\"):\n",
    "        hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name=\"hidden1\")\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name=\"hidden2\")\n",
    "        decoder_output = tf.layers.dense(hidden2, n_output_flat,\n",
    "                                         activation=tf.nn.sigmoid,\n",
    "                                         name=\"decoder_output\")\n",
    "\n",
    "    # Reconstruction Loss function\n",
    "    print('Start Reconstruction Loss')\n",
    "\n",
    "    # Flatten X\n",
    "    X_flat = tf.reshape(X, [-1, n_output_flat], name=\"X_flat\")\n",
    "    \n",
    "    \n",
    "    # We take the squared difference between the original input and the reconstructed one\n",
    "    \n",
    "    squared_difference = tf.square(X_flat - decoder_output,\n",
    "                                   name=\"squared_difference\")\n",
    "    reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                         name=\"reconstruction_loss\")\n",
    "    \n",
    "    # The loss function of our model is composed of the  reconstruction loss function (regularizer) \n",
    "    # and first loss function.\n",
    "\n",
    "    loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # We use the AdamOptimizer for this model and we want to minimize the Loss function specified earlier\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print('Start Training')\n",
    "    \n",
    "    # All the summaries and graphs are saved to those paths.\n",
    "    summaries_dir = \"./board/summaries\"\n",
    "\n",
    "    graph_dir = './board/graphs'\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "    # Start of our tensorflow session \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(summaries_dir + '/train')\n",
    "        test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "        test_writer_rotated = tf.summary.FileWriter(\n",
    "            summaries_dir + '/test_rotated')\n",
    "        test_writer_back_rand = tf.summary.FileWriter(\n",
    "            summaries_dir + '/test_back_rand')\n",
    "        test_writer_back_img = tf.summary.FileWriter(\n",
    "            summaries_dir + '/test_back_img')\n",
    "        test_writer_rot_back_img = tf.summary.FileWriter(\n",
    "            summaries_dir + '/test_rot_back_img')\n",
    "\n",
    "        graph_writer = tf.summary.FileWriter(graph_dir, sess.graph)\n",
    "\n",
    "        if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "            saver.restore(sess, checkpoint_path)\n",
    "            print(\"Starting from checkpoint\")\n",
    "        else:\n",
    "            init.run()\n",
    "            print(\"Starting from scratch\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Training of the model starts here.\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('TRAINING EPOCH: {}'.format(epoch))\n",
    "            acc_trains = []\n",
    "            for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "                \n",
    "                X_batch = x_train[(iteration-1) *\n",
    "                                  batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_train[(iteration-1) *\n",
    "                                  batch_size_train:iteration*batch_size_train]\n",
    "                \n",
    "                # Run the training operation and measure the loss:\n",
    "\n",
    "                summary, _, loss_train, acc_tra = sess.run(\n",
    "                    [merged, training_op, loss, accuracy],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch,\n",
    "                               mask_with_labels: True})\n",
    "                train_writer.add_summary(\n",
    "                    summary, iteration + n_iterations_per_epoch*epoch)\n",
    "                acc_trains.append(acc_tra)\n",
    "                print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                    iteration, n_iterations_per_epoch,\n",
    "                    iteration * 100 / n_iterations_per_epoch,\n",
    "                    loss_train),\n",
    "                    end=\"\")\n",
    "            acc_trains = np.average(acc_trains)\n",
    "            print('\\nTrain Accuracy {:.4f}%'.format(acc_trains*100))\n",
    "\n",
    "            \n",
    "            \n",
    "            # Testing of the normal MNIST starts here\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('TESTING EPOCH: {}'.format(epoch))\n",
    "\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            \n",
    "            loss_vals = []\n",
    "            acc_vals = []\n",
    "            outputs = np.zeros([])\n",
    "            \n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                X_batch = x_test[(iteration-1) *\n",
    "                                 batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test[(iteration-1) *\n",
    "                                 batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test, loss_val, acc_val, output = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer.add_summary(\n",
    "                    summary_test, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals.append(loss_val)\n",
    "                acc_vals.append(acc_val)\n",
    "                outputs = np.append(outputs, output)\n",
    "                \n",
    "                print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val = np.mean(loss_vals)\n",
    "            acc_val = np.mean(acc_vals)\n",
    "            print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val * 100, loss_val,\n",
    "                \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "            # And save the model if it improved:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, checkpoint_path)\n",
    "                best_loss_val = loss_val\n",
    "\n",
    "\n",
    "#############\n",
    "# Test model with rotated MNIST\n",
    "################\n",
    "\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('ROTATED TESTING EPOCH: {}'.format(epoch))\n",
    "\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals_rotated = []\n",
    "            acc_vals_rotated = []\n",
    "            outputs_rotated = np.zeros([])\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                X_batch = x_test_rotated[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test_rotated[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test_rotated, loss_val_rotated, acc_val_rotated, output_rotated = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer_rotated.add_summary(\n",
    "                    summary_test_rotated, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals_rotated.append(loss_val_rotated)\n",
    "                acc_vals_rotated.append(acc_val_rotated)\n",
    "                outputs_rotated = np.append(outputs_rotated, output_rotated)\n",
    "                print(\"\\rEvaluating the model_rotated: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val_rotated = np.mean(loss_vals_rotated)\n",
    "            acc_val_rotated = np.mean(acc_vals_rotated)\n",
    "            print(\"\\rEpoch: {}  Val accuracy_rotated: {:.4f}%  Loss_rotated: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val_rotated * 100, loss_val_rotated,\n",
    "                \" (improved rotation)\" if loss_val_rotated < best_loss_val_rotated else \"\"))\n",
    "\n",
    "\n",
    "#############\n",
    "# Test model with MNIST with a random background\n",
    "################\n",
    "\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('RANDOM BACK TESTING EPOCH: {}'.format(epoch))\n",
    "\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals_back_rand = []\n",
    "            acc_vals_back_rand = []\n",
    "            outputs_back_rand = np.zeros([])\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                X_batch = x_test_back_rand[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test_back_rand[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test_back_rand, loss_val_back_rand, acc_val_back_rand, output_back_rand = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer_back_rand.add_summary(\n",
    "                    summary_test_back_rand, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals_back_rand.append(loss_val_back_rand)\n",
    "                acc_vals_back_rand.append(acc_val_back_rand)\n",
    "                outputs_back_rand = np.append(\n",
    "                    outputs_back_rand, output_back_rand)\n",
    "                print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val_back_rand = np.mean(loss_vals_back_rand)\n",
    "            acc_val_back_rand = np.mean(acc_vals_back_rand)\n",
    "            print(\"\\rEpoch: {}  Val accuracy_back_rand: {:.4f}%  Loss_back_rand: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val_back_rand * 100, loss_val_back_rand,\n",
    "                \" (improved random back)\" if loss_val_back_rand < best_loss_val_back_rand else \"\"))\n",
    "\n",
    "\n",
    "#############\n",
    "# Test model with MNIST with an image as Background\n",
    "################\n",
    "\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('BACK IMG TESTING EPOCH: {}'.format(epoch))\n",
    "\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals_back_img = []\n",
    "            acc_vals_back_img = []\n",
    "            outputs_back_img = np.zeros([])\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                # .reshape([-1,tensor_depth,tensor_width, tensor_height]).transpose([0,3,2,1]).reshape(-1, tensor_depth*tensor_width*tensor_height)\n",
    "                X_batch = x_test_back_img[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test_back_img[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test_back_img, loss_val_back_img, acc_val_back_img, output_back_img = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer_back_img.add_summary(\n",
    "                    summary_test_back_img, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals_back_img.append(loss_val_back_img)\n",
    "                acc_vals_back_img.append(acc_val_back_img)\n",
    "                outputs_back_img = np.append(outputs_back_img, output_back_img)\n",
    "                print(\"\\rEvaluating the model_back_img: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val_back_img = np.mean(loss_vals_back_img)\n",
    "            acc_val_back_img = np.mean(acc_vals_back_img)\n",
    "            print(\"\\rEpoch: {}  Val accuracy_back_img: {:.4f}%  Loss_back_img: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val_back_img * 100, loss_val_back_img,\n",
    "                \" (improved back image)\" if loss_val_back_img < best_loss_val_back_img else \"\"))\n",
    "\n",
    "\n",
    "#############\n",
    "# Test of the model with a rotated MNIST set that has an image as background\n",
    "################\n",
    "\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print('ROT BACK IMG TESTING EPOCH: {}'.format(epoch))\n",
    "\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals_rot_back_img = []\n",
    "            acc_vals_rot_back_img = []\n",
    "            outputs_rot_back_img = np.zeros([])\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                # .reshape([-1,tensor_depth,tensor_width, tensor_height]).transpose([0,3,2,1]).reshape(-1, tensor_depth*tensor_width*tensor_height)\n",
    "                X_batch = x_test_rot_back_img[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "                y_batch = y_test_rot_back_img[(\n",
    "                    iteration-1)*batch_size_train:iteration*batch_size_train]\n",
    "\n",
    "                summary_test_rot_back_img, loss_val_rot_back_img, acc_val_rot_back_img, output_rot_back_img = sess.run(\n",
    "                    [merged, loss, accuracy, y_pred],\n",
    "                    feed_dict={X: X_batch,\n",
    "                               y: y_batch})\n",
    "\n",
    "                test_writer_rot_back_img.add_summary(\n",
    "                    summary_test_rot_back_img, iteration + n_iterations_per_epoch*epoch)\n",
    "\n",
    "                loss_vals_rot_back_img.append(loss_val_rot_back_img)\n",
    "                acc_vals_rot_back_img.append(acc_val_rot_back_img)\n",
    "                outputs_rot_back_img = np.append(\n",
    "                    outputs_rot_back_img, output_rot_back_img)\n",
    "                print(\"\\rEvaluating the model_rot_back_img: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_validation,\n",
    "                    iteration * 100 / n_iterations_validation),\n",
    "                    end=\" \" * 10)\n",
    "            loss_val_rot_back_img = np.mean(loss_vals_rot_back_img)\n",
    "            acc_val_rot_back_img = np.mean(acc_vals_rot_back_img)\n",
    "            print(\"\\rEpoch: {}  Val accuracy_rot_back_img: {:.4f}%  Loss_rot_back_img: {:.6f}{}\".format(\n",
    "                epoch + 1, acc_val_rot_back_img * 100, loss_val_rot_back_img,\n",
    "                \" (improved rota back img)\" if loss_val_rot_back_img < best_loss_val_rot_back_img else \"\"))\n",
    "\n",
    "        train_writer.close()\n",
    "\n",
    "        graph_writer.close()\n",
    "        test_writer_rotated.close()\n",
    "        test_writer_back_rand.close()\n",
    "        test_writer_back_img.close()\n",
    "        test_writer_rot_back_img.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T22:31:44.335639Z",
     "start_time": "2018-06-03T22:31:19.882543Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Variables\n",
      "Start Model\n",
      "Start Loss\n",
      "start decoder\n",
      "Start Reconstruction Loss\n",
      "Start Training\n",
      "Starting from scratch\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "TRAINING EPOCH: 0\n",
      "Iteration: 42/27500 (0.2%)  Loss: 0.43434"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c6923e619e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcapsnetmodel_tb_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e429cc926881>\u001b[0m in \u001b[0;36mcapsnetmodel_tb_save\u001b[0;34m(train_parameters, fix_parameters, parameters)\u001b[0m\n\u001b[1;32m    379\u001b[0m                     feed_dict={X: X_batch,\n\u001b[1;32m    380\u001b[0m                                \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                                mask_with_labels: True})\n\u001b[0m\u001b[1;32m    382\u001b[0m                 train_writer.add_summary(\n\u001b[1;32m    383\u001b[0m                     summary, iteration + n_iterations_per_epoch*epoch)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "capsnetmodel_tb_save(train_params, fix_params, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
